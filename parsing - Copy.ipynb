{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_LOGIN = '**********'\n",
    "USER_PASSWORD = '**********'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = DesiredCapabilities().CHROME\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "\n",
    "# waiting for the page to load\n",
    "time.sleep(3.5)\n",
    "\n",
    "# entering username\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "\n",
    "# In case of an error, try changing the element\n",
    "# tag used here.\n",
    "\n",
    "# Enter Your Email Address\n",
    "username.send_keys(USER_LOGIN)\n",
    "\n",
    "# entering password\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "# In case of an error, try changing the element\n",
    "# tag used here.\n",
    "\n",
    "# Enter Your Password\n",
    "pword.send_keys(USER_PASSWORD)\n",
    "\n",
    "# Clicking on the log in button\n",
    "# Format (syntax) of writing XPath -->\n",
    "# //tagname[@attribute='value']\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users (link):\n",
    "    # Open search page\n",
    "    driver.get(link)\n",
    "    profile_urls = []\n",
    "\n",
    "    # NUM_PAGES_TO_PARSE = 12\n",
    "    NUM_PAGES_TO_PARSE = 12\n",
    "\n",
    "    # Iterate over pages of search results\n",
    "    # to collect profile urls\n",
    "    for i in range(NUM_PAGES_TO_PARSE):\n",
    "        search_result_links = driver.find_elements(By.CLASS_NAME, \"app-aware-link\")\n",
    "        for link in search_result_links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if 'linkedin.com/in' in href:\n",
    "                profile_urls.append(href)\n",
    "\n",
    "        # HACK TO SEE NEXT BUTTON BY SELENIUM\n",
    "        # We scroll down the page to make element visible for Selenium\n",
    "        SCROLL_PAUSE_TIME = np.random.randint(1, 3)\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.execute_script(\"document.body.style.zoom='20%'\")\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, (document.body.scrollHeight/2));\")\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            driver.execute_script(\"window.scrollTo(0, (document.body.scrollHeight));\")\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        driver.execute_script(\"document.body.style.zoom='100%'\")\n",
    "\n",
    "        next_button = driver.find_element(By.CLASS_NAME, 'artdeco-pagination__button--next')\n",
    "        next_button.click()\n",
    "        time.sleep(5)\n",
    "\n",
    "    profile_list = list(set(profile_urls))\n",
    "    return profile_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    users_df = pd.read_csv('users.csv',index_col=[0])\n",
    "except FileNotFoundError:\n",
    "    profile_urls = []\n",
    "    links = ['https://www.linkedin.com/search/results/people/?keywords=Senior%20developer%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%22602%22%2C%22764%22%2C%221383%22%2C%223166%22%2C%2255798%22%2C%223498%22%2C%225171%22%2C%2226904%22%2C%2256683%22%5D&sid=H47',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=senior%20analyst%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%22602%22%2C%22764%22%2C%221383%22%2C%223166%22%2C%2255798%22%2C%223498%22%2C%225171%22%2C%2226904%22%2C%2256683%22%5D&sid=p',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=lead%20analyst%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%226177%22%2C%22602%22%2C%22764%22%2C%221383%22%2C%223166%22%2C%223498%22%2C%2226904%22%2C%225171%22%2C%2255798%22%2C%2256683%22%5D&sid=tA6',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=senior%20engineer%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%226177%22%2C%22602%22%2C%22764%22%2C%221383%22%2C%223166%22%2C%223498%22%2C%2226904%22%2C%225171%22%2C%2255798%22%2C%2256683%22%5D&sid=HRM',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=lead%20engineer%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%226177%22%2C%22602%22%2C%22764%22%2C%221383%22%2C%223166%22%2C%223498%22%2C%2226904%22%2C%225171%22%2C%2255798%22%2C%2256683%22%5D&sid=_6z',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=senior%20qa%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%2250321%22%2C%22602%22%2C%22764%22%2C%221383%22%2C%226177%22%2C%223166%22%2C%2255798%22%2C%223498%22%2C%225171%22%2C%2226904%22%2C%2256683%22%5D&sid=apf',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=senior%20ui%20ux%20designer%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%22207%22%2C%22272%22%2C%22602%22%2C%22764%22%2C%225171%22%2C%2255798%22%2C%226177%22%2C%221383%22%2C%2226904%22%2C%223166%22%2C%223498%22%2C%2250321%22%2C%2256683%22%5D&sid=ySg',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=senior%20product%20manager%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%22207%22%2C%22272%22%2C%22602%22%2C%22764%22%2C%225171%22%2C%2255798%22%2C%226177%22%2C%221383%22%2C%2226904%22%2C%223166%22%2C%223498%22%2C%2250321%22%2C%2256683%22%5D&sid=b60',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=senior%20project%20manager%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%22207%22%2C%22272%22%2C%22602%22%2C%22764%22%2C%225171%22%2C%2255798%22%2C%226177%22%2C%221383%22%2C%2226904%22%2C%223166%22%2C%223498%22%2C%2250321%22%2C%2256683%22%5D&sid=Xxx',\n",
    "    'https://www.linkedin.com/search/results/people/?keywords=senior%20data%20scientist%20NOT%20Ukraine%20NOT%20%D0%A3%D0%BA%D1%80%D0%B0%D0%B8%D0%BD%D0%B0&origin=GLOBAL_SEARCH_HEADER&profileLanguage=%5B%22ru%22%5D&serviceCategory=%5B%226177%22%2C%22602%22%2C%22764%22%2C%221383%22%2C%223166%22%2C%223498%22%2C%2226904%22%2C%225171%22%2C%2255798%22%2C%2256683%22%5D&sid=iVI']\n",
    "\n",
    "    for i in links:\n",
    "        profile_urls.append(get_users(i))\n",
    "\n",
    "    users_df = pd.DataFrame({'link': profile_urls})\n",
    "    # users_df.to_csv('users.csv')\n",
    "    temp_df = []\n",
    "    for i in range(len(users_df)):\n",
    "        for j in users_df['link'][i].split(','):\n",
    "            temp_df.append(j)\n",
    "\n",
    "    users_df = pd.DataFrame({'link': temp_df})\n",
    "    users_df.to_csv('users.csv')\n",
    "    users_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2096 entries, 0 to 2095\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   link    2096 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 32.8+ KB\n"
     ]
    }
   ],
   "source": [
    "users_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_print_user_experience(driver, posts_url):\n",
    "    driver.get(posts_url)\n",
    "\n",
    "    #Simulate scrolling to capture all posts\n",
    "    SCROLL_PAUSE_TIME = 1.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # We can adjust this number to get more posts\n",
    "    NUM_SCROLLS = 5\n",
    "\n",
    "    for i in range(NUM_SCROLLS):\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Parsing posts\n",
    "    src = driver.page_source\n",
    "\n",
    "    # Now using beautiful soup\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    # soup.prettify()\n",
    "\n",
    "    # experience = soup.find_all('div', {'class':'display-flex full-width'})\n",
    "    # print(experience)\n",
    "\n",
    "    # find the experience in years \n",
    "    xp_in_years = soup.find_all('span', {'class':'t-14 t-normal t-black--light'})\n",
    "    xp_in_years=str(xp_in_years).replace('<span class=\"t-14 t-normal t-black--light\">',' ')\n",
    "    xp_in_years=str(xp_in_years).replace('<span aria-hidden=\"true\">','.') \n",
    "    xp_in_years=str(xp_in_years).replace('<!-- -->','.')\n",
    "    xp_in_years=str(xp_in_years).replace('</span>','')\n",
    "    xp_in_years=str(xp_in_years).replace('<span class=\"visually-hidden\">',' ')\n",
    "    xp_in_years = xp_in_years.split('.')\n",
    "\n",
    "    correct_xp=[]\n",
    "    years_list = ['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023']\n",
    "    correct_xp=[]\n",
    "    for i in xp_in_years:\n",
    "        positive = 0\n",
    "        if len(i) > 5:\n",
    "            for j in  years_list:\n",
    "                if j in i:\n",
    "                    positive=1\n",
    "            if positive == 1:\n",
    "                correct_xp.append(i)\n",
    "    \n",
    "    # duplicates drop\n",
    "    xp_in_years =[]\n",
    "    for i in range(len(correct_xp)):\n",
    "        if (i+1)%2 == 0:\n",
    "            continue\n",
    "        else: xp_in_years.append(correct_xp[i])\n",
    "\n",
    "    # split on date and duration\n",
    "    correct_xp=[]\n",
    "    for i in xp_in_years:\n",
    "        correct_xp.append(i.split('·'))\n",
    "    \n",
    "    # drop duration\n",
    "    xp_in_years = []\n",
    "    for i in correct_xp:\n",
    "        xp_in_years.append(i[0])\n",
    "\n",
    "    # find the history of experience in companies\n",
    "    xp_company = soup.find_all('span', {'class':'t-14 t-normal'})\n",
    "    xp_company=str(xp_company).replace('<span class=\"t-14 t-normal\">\\n','')\n",
    "    xp_company=str(xp_company).replace('<span aria-hidden=\"true\">','.') \n",
    "    xp_company=str(xp_company).replace('<!-- -->','.')\n",
    "    xp_company=str(xp_company).replace('</span>','')\n",
    "    xp_company=str(xp_company).replace('<span class=\"visually-hidden\">','')\n",
    "    xp_company = xp_company.split('.')\n",
    "\n",
    "    # filtering\n",
    "    correct_company=[]\n",
    "    for i in xp_company:\n",
    "        if len(i) > 5:\n",
    "            correct_company.append(i)\n",
    "    \n",
    "    # duplicates drop\n",
    "    xp_company =[]\n",
    "    for i in range(len(correct_company)):\n",
    "        if (i+1)%2 == 0:\n",
    "            continue\n",
    "        else: \n",
    "            xp_company.append(correct_company[i])\n",
    "\n",
    "    # find the history of experience in companies\n",
    "    xp_position = soup.find_all('div', {'class':'display-flex align-items-center mr1 t-bold'})\n",
    "    xp_position=str(xp_position).replace('<div class=\"display-flex align-items-center mr1 t-bold\">','')\n",
    "    xp_position=str(xp_position).replace('<span aria-hidden=\"true\">','.') \n",
    "    xp_position=str(xp_position).replace('<!-- -->','.')\n",
    "    xp_position=str(xp_position).replace('</span>','')\n",
    "    xp_position=str(xp_position).replace('</div>','')\n",
    "    xp_position=str(xp_position).replace('<span class=\"visually-hidden\">','')\n",
    "    xp_position = xp_position.split('.')\n",
    "\n",
    "    # filtering len >5\n",
    "    correct_position=[]\n",
    "    for i in xp_position:\n",
    "        if len(i) > 5:\n",
    "            correct_position.append(i)\n",
    "\n",
    "    # duplicates drop\n",
    "    xp_position =[]\n",
    "    for i in range(len(correct_position)):\n",
    "        if (i+1)%2 == 0:\n",
    "            continue\n",
    "        else: \n",
    "            xp_position.append(correct_position[i])\n",
    "\n",
    "    return(xp_in_years,xp_company,xp_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_append(required_len,df_to_update):\n",
    "    if required_len > len(df_to_update):\n",
    "        for i in range(required_len - len(df_to_update)):\n",
    "            df_to_update.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_print_profile_xp (driver, profile_url):\n",
    "    \n",
    "    driver.get(profile_url)    # this will open the link    \n",
    "\n",
    "    # Extracting data from page with BeautifulSoup\n",
    "    src = driver.page_source\n",
    "\n",
    "    # Now using beautiful soup\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "    # Extracting the HTML of the complete introduction box\n",
    "    # that contains the name, company name, and the location\n",
    "    intro = soup.find('div', {'class': 'pv-text-details__left-panel'})\n",
    "\n",
    "    # In case of an error, try changing the tags used here.\n",
    "    name_loc = intro.find(\"h1\")\n",
    "\n",
    "    # Extracting the Name\n",
    "    name = name_loc.get_text().strip()\n",
    "    # strip() is used to remove any extra blank spaces\n",
    "\n",
    "    works_at_loc = intro.find(\"div\", {'class': 'text-body-medium'})\n",
    "\n",
    "    # this gives us the HTML of the tag in which the Company Name is present\n",
    "    # Extracting the Company Name\n",
    "    works_at = works_at_loc.get_text().strip()\n",
    "\n",
    "    # print(\"Name -->\",  name,'\\n', \"Works At -->\", works_at)\n",
    "\n",
    "    EXPERIENCE_URL_SFX = '/details/experience/'\n",
    "\n",
    "    # Get current url from browser\n",
    "    cur_profile_url = driver.current_url\n",
    "\n",
    "    # Parse experience \n",
    "    xp_years_list, xp_company,xp_position = get_and_print_user_experience(driver, cur_profile_url + EXPERIENCE_URL_SFX)\n",
    "\n",
    "    ttl_len = max(len(xp_years_list),len(xp_company),len(xp_position))\n",
    "\n",
    "    len_append(ttl_len,xp_years_list)\n",
    "    len_append(ttl_len,xp_company)\n",
    "    len_append(ttl_len,xp_position)\n",
    "\n",
    "    name_list=[]\n",
    "    link_list = []\n",
    "    for i in range(ttl_len):\n",
    "        name_list.append(name)\n",
    "        link_list.append(cur_profile_url)\n",
    "\n",
    "    xp_user_df = pd.DataFrame({'name':name_list,\n",
    "                            'position': xp_position,\n",
    "                            'experience': xp_years_list, \n",
    "                            'company': xp_company,'link':link_list})\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    return(xp_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>position</th>\n",
       "      <th>experience</th>\n",
       "      <th>company</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roman Grinovski</td>\n",
       "      <td>Frontend Engineer</td>\n",
       "      <td>Mar 2023 - Present</td>\n",
       "      <td>Spacelift · Full-time</td>\n",
       "      <td>https://www.linkedin.com/in/roman-grinovski-0a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roman Grinovski</td>\n",
       "      <td>Frontend Engineer</td>\n",
       "      <td>Oct 2021 - Mar 2023</td>\n",
       "      <td>Via · Self-employed</td>\n",
       "      <td>https://www.linkedin.com/in/roman-grinovski-0a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roman Grinovski</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Jul 2020 - Oct 2021</td>\n",
       "      <td>UI Bakery</td>\n",
       "      <td>https://www.linkedin.com/in/roman-grinovski-0a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roman Grinovski</td>\n",
       "      <td>Frontend Engineer</td>\n",
       "      <td>Sep 2019 - Oct 2021</td>\n",
       "      <td>Akveo · Full-time</td>\n",
       "      <td>https://www.linkedin.com/in/roman-grinovski-0a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roman Grinovski</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Dec 2017 - Aug 2019</td>\n",
       "      <td>Aligned Code</td>\n",
       "      <td>https://www.linkedin.com/in/roman-grinovski-0a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vardan Galstyan</td>\n",
       "      <td>Web Application Developer</td>\n",
       "      <td>Dec 2013 - Jan 2020</td>\n",
       "      <td>BlueNet Armenia · Full-time</td>\n",
       "      <td>https://www.linkedin.com/in/vardan-galstyan-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vardan Galstyan</td>\n",
       "      <td>Senior Frontend Developer</td>\n",
       "      <td>Jan 2018 - May 2018</td>\n",
       "      <td>Self Employed - Freelance · Freelance</td>\n",
       "      <td>https://www.linkedin.com/in/vardan-galstyan-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vardan Galstyan</td>\n",
       "      <td>Web Software Developer</td>\n",
       "      <td>Jul 2015 - Jan 2018</td>\n",
       "      <td>Digitain · Full-time</td>\n",
       "      <td>https://www.linkedin.com/in/vardan-galstyan-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vardan Galstyan</td>\n",
       "      <td>Junior Web Developer</td>\n",
       "      <td>Sep 2013 - Dec 2013</td>\n",
       "      <td>AIST Global</td>\n",
       "      <td>https://www.linkedin.com/in/vardan-galstyan-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vardan Galstyan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Instigate CJSC</td>\n",
       "      <td>https://www.linkedin.com/in/vardan-galstyan-12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                   position            experience  \\\n",
       "0   Roman Grinovski          Frontend Engineer   Mar 2023 - Present    \n",
       "1   Roman Grinovski          Frontend Engineer  Oct 2021 - Mar 2023    \n",
       "2   Roman Grinovski          Software Engineer  Jul 2020 - Oct 2021    \n",
       "3   Roman Grinovski          Frontend Engineer  Sep 2019 - Oct 2021    \n",
       "4   Roman Grinovski          Software Engineer  Dec 2017 - Aug 2019    \n",
       "..              ...                        ...                   ...   \n",
       "5   Vardan Galstyan  Web Application Developer  Dec 2013 - Jan 2020    \n",
       "6   Vardan Galstyan  Senior Frontend Developer  Jan 2018 - May 2018    \n",
       "7   Vardan Galstyan     Web Software Developer  Jul 2015 - Jan 2018    \n",
       "8   Vardan Galstyan       Junior Web Developer  Sep 2013 - Dec 2013    \n",
       "9   Vardan Galstyan                                                    \n",
       "\n",
       "                                  company  \\\n",
       "0                   Spacelift · Full-time   \n",
       "1                     Via · Self-employed   \n",
       "2                               UI Bakery   \n",
       "3                       Akveo · Full-time   \n",
       "4                            Aligned Code   \n",
       "..                                    ...   \n",
       "5             BlueNet Armenia · Full-time   \n",
       "6   Self Employed - Freelance · Freelance   \n",
       "7                    Digitain · Full-time   \n",
       "8                             AIST Global   \n",
       "9                          Instigate CJSC   \n",
       "\n",
       "                                                 link  \n",
       "0   https://www.linkedin.com/in/roman-grinovski-0a...  \n",
       "1   https://www.linkedin.com/in/roman-grinovski-0a...  \n",
       "2   https://www.linkedin.com/in/roman-grinovski-0a...  \n",
       "3   https://www.linkedin.com/in/roman-grinovski-0a...  \n",
       "4   https://www.linkedin.com/in/roman-grinovski-0a...  \n",
       "..                                                ...  \n",
       "5   https://www.linkedin.com/in/vardan-galstyan-12...  \n",
       "6   https://www.linkedin.com/in/vardan-galstyan-12...  \n",
       "7   https://www.linkedin.com/in/vardan-galstyan-12...  \n",
       "8   https://www.linkedin.com/in/vardan-galstyan-12...  \n",
       "9   https://www.linkedin.com/in/vardan-galstyan-12...  \n",
       "\n",
       "[597 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collecting info on users experience\n",
    "count = 0\n",
    "for profile_url in users_df.iloc[0:100,0]:\n",
    "    user_xp = get_and_print_profile_xp(driver, profile_url)\n",
    "    if count == 0:\n",
    "        user_xp_df_out = user_xp\n",
    "    else:\n",
    "        user_xp_df_out = pd.concat([user_xp_df_out,user_xp])\n",
    "    count+=1\n",
    "\n",
    "user_xp_df_out.to_excel('users_and_experience.xlsx')\n",
    "user_xp_df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_print_user_posts(driver, posts_url):\n",
    "    driver.get(posts_url)\n",
    "\n",
    "    #Simulate scrolling to capture all posts\n",
    "    SCROLL_PAUSE_TIME = 1.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # We can adjust this number to get more posts\n",
    "    NUM_SCROLLS = 5\n",
    "\n",
    "    for i in range(NUM_SCROLLS):\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Parsing posts\n",
    "    src = driver.page_source\n",
    "\n",
    "    # Now using beautiful soup\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    # soup.prettify()\n",
    "\n",
    "    posts = soup.find_all('li', class_='profile-creator-shared-feed-update__container')\n",
    "    # print(posts)\n",
    "\n",
    "    posts_out = []\n",
    "    reactions_out = []\n",
    "\n",
    "    # print(f'Number of posts: {len(posts)}')\n",
    "    for post_src in posts:\n",
    "        post_text_div = post_src.find('div', {'class': 'feed-shared-update-v2__description-wrapper mr2'})\n",
    "        \n",
    "\n",
    "        # if post_text_div is None:\n",
    "        #     print(post_src)\n",
    "\n",
    "        if post_text_div is not None:\n",
    "            post_text = post_text_div.find('span', {'dir': 'ltr'})\n",
    "        else:\n",
    "            post_text = None\n",
    "            posts_out.append(post_text)\n",
    "\n",
    "        # If post text is found\n",
    "        if post_text is not None:\n",
    "            post_text = post_text.get_text().strip()\n",
    "            posts_out.append(post_text)\n",
    "            # print(f'Post text: {post_text}')\n",
    "\n",
    "        reaction_cnt = post_src.find('span', {'class': 'social-details-social-counts__reactions-count'})\n",
    "\n",
    "        # If number of reactions is written as text\n",
    "        # It has different class name\n",
    "        if reaction_cnt is None:\n",
    "            reaction_cnt = post_src.find('span', {'class': 'social-details-social-counts__social-proof-text'})\n",
    "            reactions_out.append(0)\n",
    "\n",
    "        if reaction_cnt is not None:\n",
    "            reaction_cnt = reaction_cnt.get_text().strip()\n",
    "            reactions_out.append(reaction_cnt)\n",
    "            # print(f'Reactions: {reaction_cnt}')\n",
    "\n",
    "    return posts_out,reactions_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_print_profile_info(driver, profile_url):\n",
    "    driver.get(profile_url)        # this will open the link\n",
    "\n",
    "    # Extracting data from page with BeautifulSoup\n",
    "    src = driver.page_source\n",
    "\n",
    "    # Now using beautiful soup\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "    # Extracting the HTML of the complete introduction box\n",
    "    # that contains the name, company name, and the location\n",
    "    intro = soup.find('div', {'class': 'pv-text-details__left-panel'})\n",
    "\n",
    "    # In case of an error, try changing the tags used here.\n",
    "    name_loc = intro.find(\"h1\")\n",
    "\n",
    "    # Extracting the Name\n",
    "    name = name_loc.get_text().strip()\n",
    "    # strip() is used to remove any extra blank spaces\n",
    "\n",
    "    works_at_loc = intro.find(\"div\", {'class': 'text-body-medium'})\n",
    "\n",
    "    # this gives us the HTML of the tag in which the Company Name is present\n",
    "    # Extracting the Company Name\n",
    "    works_at = works_at_loc.get_text().strip()\n",
    "\n",
    "    # print(\"Name -->\",  name,'\\n', \"Works At -->\", works_at)\n",
    "\n",
    "    POSTS_URL_SUFFIX = 'recent-activity/all/'\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Get current url from browser\n",
    "    cur_profile_url = driver.current_url\n",
    "\n",
    "    # print(cur_profile_url+POSTS_URL_SUFFIX)\n",
    "\n",
    "    # Parse posts\n",
    "    posts, reactions = get_and_print_user_posts(driver, cur_profile_url + POSTS_URL_SUFFIX)\n",
    "    # print(posts, reactions)\n",
    "\n",
    "    if len(posts) > len(reactions):\n",
    "        ttl_len = len(posts)\n",
    "    else:\n",
    "        ttl_len = len(reactions)\n",
    "\n",
    "    name_list=[]\n",
    "    works_at_list = []\n",
    "    link_list = []\n",
    "    for i in range(ttl_len):\n",
    "        name_list.append(name)\n",
    "        works_at_list.append(works_at)\n",
    "        link_list.append(cur_profile_url + POSTS_URL_SUFFIX)\n",
    "\n",
    "    posts_df = pd.DataFrame({'name':name_list,'works_at':works_at_list, 'posts': posts, 'reactions': reactions})\n",
    "    return(posts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse profile urls\n",
    "count = 0\n",
    "for profile_url in users_df.iloc[0:10,0]:\n",
    "    user_posts = get_and_print_profile_info(driver, profile_url)\n",
    "    if count == 0:\n",
    "        posts_df_out = user_posts\n",
    "    else:\n",
    "        posts_df_out = pd.concat([posts_df_out,user_posts])\n",
    "    time.sleep(2)\n",
    "    count+=1\n",
    "\n",
    "posts_df_out\n",
    "posts_df_out.to_excel('posts.xlsx')\n",
    "\n",
    "# close the Chrome browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_out.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
